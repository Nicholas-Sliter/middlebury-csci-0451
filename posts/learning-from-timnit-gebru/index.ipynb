{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Learning from Dr. Timnit Gebru\n",
    "author: Nicholas Sliter\n",
    "date: '2023-04-19'\n",
    "image: \"images/sigmoid.webp\"\n",
    "description: \"A discussion with Dr. Gebru\"\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    code-summary: \"Show the code\"\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Introduction\n",
    "\n",
    "On Monday, April 23rd 2023, a preeminent AI-ethicist, Dr. Timnit Gebru, is speaking at Middlebury. She will virtually visit ML classes and later give a public talk. \n",
    "\n",
    "Dr. Gebru is an AI-ethicist whose research focuses on bias and fairness in machine learning. Her recent work has famously focused on the ethics of large language models. She was an early critic of LLMs and their potential for bias, harm, and negative environmental impact. She has also been a vocal critic of the lack of diversity in the field of AI. \n",
    "\n",
    "Having Dr. Gebru attend Middlebury is an excellent opportunity to focus on the ever exigent topic of AI ethics. With AI (read LLMs) going mainstream with applications like ChatGPT, I don't think this talk could come at a better time. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Previous Talks\n",
    "In 2020, Dr. Gebru gave a talk as part of a Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision at the conference on Computer Vision and Pattern Recognition 2020. \n",
    "\n",
    "To prepare for Dr. Gebru's visit, I watched a recording of this talk. I found it to be a very informative and engaging talk. I think it is a great introduction to the topic of AI ethics.\n",
    "\n",
    "She discusses the impact of AI on marginalized groups. She starts with how bias in facial recognition systems in hiring applications (HireVue) could explicitly discriminate against faces it does not recognize as well. She questions HireVue's statement that its model can predict the internal state of a candidate and criticizes the lack of transparency in the model. She also discusses how facial recognition systems employed by the police can be used to surveil and then suppress protests. Her examples show misuse of these systems in incredibly harmful ways that damage the foundations of our society.  \n",
    "\n",
    "She also discuses how data is politics and an 'unbiased' model (in relation to its dataset) is a political statement. \n",
    "\n",
    "She addresses how data diversity issues cause harm. Her examples show how lack of diverse training data causes worse performance on diverse test data. And this can have large consequences for marginalized groups (see the HireVue example above). She also relates this to work outside of AI by discussing examples of clinic trials causing harm by excluding women or people of color. \n",
    "\n",
    "She also discusses data privacy and ethics in data collections in relation to computer vision. Since Flickr is commonly used in computer vision datasets, she discusses the lack of consent given by the original posters of the images. She exemplifies this by describing how researchers train facial recognition systems on images of people without their consent.\n",
    "\n",
    "She then discusses the idea of fairness and how this relates to the lack of diversity in the field of AI. She then makes a call to action for the field of AI to take ownership of the problems it creates and to take responsibility for fixing them. She calls for scientists to be activists that strive for fairness by making their models less biased AND by not working on models that are harmful regardless of their bias (e.g. facial recognition systems used by the police). \\"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Questions for Dr. Gebru\n",
    "\n",
    "* With the increasing prevalence of LLMs in daily life, how do you think daily usage of these models will change us (humans, collectively)?\n",
    "* With regards to auditing bias in LLMs, we know it is very difficult to audit training data. So what methods do you think are most effective in auditing bias in LLMs? How can we verify that a model is not biased? Do you think that promote-based approaches are viable and effective?\n",
    "* With regards to multi-language LLMs, how do you think we can ensure that these models are not biased towards high resource languages (eg. English)? And how do we know if the internal representations of these models are shared between languages?\n",
    "* Since a model being unbiased is not sufficient to prevent harm, what steps can we take to ensure that LLMs are not used to harm stigmatized groups or perpetuate existing biases? Is this the job of the model developer or the user?\n",
    "* What data can we train with? Some would say that any data a human can see on the internet is fair game. Others would say that we should only train on data that we have explicit consent to use. What do you think?\n",
    "\n",
    "\n",
    "I will admit, many of these questions are inspired by my amazing partner, Katelyn Mei, a PhD student in AI Ethics at UW. She will be presenting at ACM FAccT 2023 on auditing bias against stigmatized groups in MLMs. Her paper is roughly titled: \"Bias against 93 stigmatized groups in masked language models\"."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
