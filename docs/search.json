[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS451 Course Blog",
    "section": "",
    "text": "Exploring logistic regression and gradient descent\n\n\n\n\n\n\nMar 27, 2023\n\n\nNicholas Sliter\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nExploring a basic Perceptron algorithm in Python.\n\n\n\n\n\n\nFeb 26, 2023\n\n\nNicholas Sliter\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/logistic-regression-and-gradient-descent/index.html",
    "href": "posts/logistic-regression-and-gradient-descent/index.html",
    "title": "Logistic Regression & Gradient Descent",
    "section": "",
    "text": "Source LogisticRegression.py\nThis blog post outlines an implementation of linear classification via logistic regression with gradient descent, stochastic gradient descent, and stochastic gradient descent with momentum. I will address the implementations and differing convergence rates of each method.\nLogistic Regression is based around the sigmoid function, a convex function defined as: \\[\\sigma(x) = \\frac{1}{1 + e^{-x}}\\].\nUsing numpy, we define a vectorized sigmoid function as:"
  },
  {
    "objectID": "posts/logistic-regression-and-gradient-descent/index.html#learning-rate",
    "href": "posts/logistic-regression-and-gradient-descent/index.html#learning-rate",
    "title": "Logistic Regression & Gradient Descent",
    "section": "Learning Rate",
    "text": "Learning Rate\nNow naturally, we’d like our algorithm to converge to a global minimum. And as it stands, this is not guaranteed. We can however, make it more likely by using a learning rate \\(\\alpha \\in (0,1)\\) to control the step size of our gradient descent algorithm. The intuition behind this is that if we take a step in the direction of the gradient, we will move closer to the global minimum. But if we step too far, we may overshoot the global minimum and end up somewhere worse.\n\n\n\nWhen alpha is too large we can bounce around the global minimum and never reach it. stats.stackexchange.com\n\n\nBy making smaller steps, we decrease the likelihood of overshooting the global minimum. By sufficiently decreasing our step size, we can guarantee convergence at the cost of training time. Our new gradient descent algorithm is given as:\n\\[ w_{i+1} = w_{i} - \\alpha \\nabla L(w) \\]"
  },
  {
    "objectID": "posts/logistic-regression-and-gradient-descent/index.html#stochastic-gradient-descent",
    "href": "posts/logistic-regression-and-gradient-descent/index.html#stochastic-gradient-descent",
    "title": "Logistic Regression & Gradient Descent",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\nThis equation can be quite costly to compute, especially if we have a large number of samples. We can reduce the cost of this computation by using a batch \\(b\\) to compute the gradient of the loss function over a subset of the data. The intuition behind this is that the gradient of the loss function over a subset of the data is probably a good approximation of the gradient of the loss function over the entire dataset. And taking a step in the direction of this approximation should bring us closer to the global minimum.\nThis is known as stochastic gradient descent. The new algorithm is given as:\n\\[ w_{i+1} = w_{i} - \\alpha \\nabla L(w)_b \\]\nwhere \\(b\\) is a random subset of the data. Then we can iterate over the entire dataset by repeating this process for each batch."
  },
  {
    "objectID": "posts/logistic-regression-and-gradient-descent/index.html#stochastic-gradient-descent-with-momentum",
    "href": "posts/logistic-regression-and-gradient-descent/index.html#stochastic-gradient-descent-with-momentum",
    "title": "Logistic Regression & Gradient Descent",
    "section": "Stochastic Gradient Descent with Momentum",
    "text": "Stochastic Gradient Descent with Momentum\nThe problem with stochastic gradient descent is that it can be noisy. This is because we are taking a step in the direction of the gradient of the loss function over a random subset of the data. This can lead to the algorithm taking certain steps that are orthogonal or even opposite to the direction of the global minimum, and thus increase the noise at each step and increasing training time. We can reduce this noise by using momentum. The idea behind momentum is that we can keep track of the direction of the previous step and use this to help us take a step in the right direction. We can do this by adding a momentum term to our gradient descent algorithm, where \\(\\beta \\in (0,1)\\) is the momentum coefficient. This gives us the following equation:\n\\[ w_{i+1} = w_{i} - (\\alpha \\nabla L(w)_b + \\beta (w_{i} - w_{i-1})) \\]"
  },
  {
    "objectID": "posts/logistic-regression-and-gradient-descent/index.html#gradient-descent-implementations",
    "href": "posts/logistic-regression-and-gradient-descent/index.html#gradient-descent-implementations",
    "title": "Logistic Regression & Gradient Descent",
    "section": "Gradient Descent Implementations",
    "text": "Gradient Descent Implementations\nFor ease of implementation, gradient descent and its stochastic variants are implemented in two functions LogisticRegression.fit and LogisticRegression.fit_stochastic.\nLogisticRegression.fit_stochastic handles both stochastic gradient descent and stochastic gradient descent with momentum. As only difference between the two is the addition of the momentum term.\nWe define our model fit via gradient descent as:\ndef __fit(self, X, y, alpha, epochs) -> None:\n        ''' Internal method to fit the model\n        @param X: array of shape (n_samples, n_features + 1)\n        @param y: array of shape (n_samples,)\n        @param alpha: learning rate\n        @param epochs: number of epochs\n        @return: None\n        '''\n        if self.w is not None: raise Exception('Model is already trained')\n\n        _, n_features = X.shape\n        self.w = self.__get_random_weights(n_features)\n        self.__record_history(self.__loss(X, y), self.__score(X, y))\n\n        for epoch in range(epochs):\n        \n            y_pred = self.__predict(X)\n\n            # Update weights       \n            grad = self.__grad(X, y, y_pred)\n            update_amt  = (alpha * grad)\n            self.w -= update_amt\n\n            # Record loss and check for convergence\n            loss = self.__loss(X, y)\n            score = self.__score(X, y)\n\n            self.__record_history(loss, score)\n            if (np.linalg.norm(update_amt) <= self.epsilon): break\n        print(f\"Fit model with gradient descent in {epoch+1}/{epochs}(max) epochs.\")\nThe code for this follows from the above equation. We initialize our weights to random values, and then update them using the gradient of the loss function. We then record the loss and score of the model and check for convergence. If the model has converged, we break out of the loop. Otherwise, we continue to the next epoch.\nWe do this until convergence or until we reach the maximum number of epochs.\nSimilarly, we define our model fit via stochastic gradient descent as:\n\n def __fit_stochastic(self, X, y, alpha, epochs, batch_size, momentum=False, beta=0.8) -> None:\n        ''' Internal method to fit the model using stochastic gradient descent\n        @param X: array of shape (n_samples, n_features + 1)\n        @param y: array of shape (n_samples,)\n        @param alpha: learning rate\n        @param epochs: number of epochs\n        @param batch_size: size of batch\n        @param momentum: boolean to use momentum\n        @return: None\n        '''\n        beta: float = 1*(momentum*beta) # 0 when momentum is false\n\n        if self.w is not None: raise Exception('Model is already trained')\n\n        n, n_features = X.shape\n        self.w = self.__get_random_weights(n_features)\n        self.__record_history(self.__loss(X, y), self.__score(X, y))\n\n        for epoch in np.arange(epochs):\n            order = np.arange(n)\n            self.__rs.shuffle(order)\n\n            previous_update_vector = np.zeros(n_features)\n            for batch in np.array_split(order, n // batch_size + 1):\n                x_batch = X[batch,:]\n                y_batch = y[batch]\n                y_batch_pred = self.__predict(x_batch)\n                grad = self.__grad(x_batch, y_batch, y_batch_pred) \n\n                # Stochastic Gradient step with optional momentum\n                momentum_term =  (beta * (previous_update_vector))\n                update_vector = (alpha * grad) + momentum_term\n\n                previous_update_vector = update_vector\n\n                self.w -= update_vector\n\n            # Record loss and check for convergence\n            loss = self.__loss(X, y)\n            score = self.__score(X, y)\n\n            self.__record_history(loss, score)\n            if (np.linalg.norm(previous_update_vector) <= self.epsilon): break\n        print(f\"Fit model with stochastic gradient descent in {epoch+1}/{epochs}(max) epochs.\")\nHere we can see the subtle difference between gradient descent and its stochastic variants. We initialize our weights to random values, and then update them using the gradient of the loss function over a random subset of the data. We repeat until we have processed the entire dataset. Then, we repeat this process for the number of epochs. At the end of each epoch, we record the loss and score of the model and check for convergence. If the model has converged, we break out of the loop. Otherwise, we continue to the next epoch (and each batch within).\nWe define our momentum coefficient as beta = 0.8 which is a common convention."
  },
  {
    "objectID": "posts/logistic-regression-and-gradient-descent/index.html#examples",
    "href": "posts/logistic-regression-and-gradient-descent/index.html#examples",
    "title": "Logistic Regression & Gradient Descent",
    "section": "Examples:",
    "text": "Examples:\n\nExample 1: Large alpha leading to divergence\nConsider the following example where we set the learning rate to a large value. As earlier claimed, this can lead to divergence of the model.\nWe will test this claim on the make_moons dataset from sklearn.datasets. This dataset is not linearly separable.\n\n\nShow the code\n# Example: alpha too large\nfrom sklearn.datasets import make_moons\n\nX_dnc, y_dnc = make_moons(n_samples=400, noise=0.15, random_state=seed)\n\nfig = plt.scatter(X_dnc[:,0], X_dnc[:,1], c = y_dnc)\n\nNone # To suppress Matplotlib output\n\n\n\n\n\nWe will fit our model using normal gradient descent. We will set the learning rate to the obscenely large value of 29. Observe how the loss history of our model fails to converge to a solution over the course of 2000 epochs.\n\n\nShow the code\nLR = LogisticRegression()\nLR.fit(X_dnc, y_dnc, alpha = 29, epochs = 2000)\n\nnum_steps = len(LR.loss_history)\n\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\n\nax= plt.gca()\n\nplt.show()\n\nNone # To suppress Matplotlib output\n\n\nFit model with gradient descent in 2000/2000(max) epochs.\n\n\n\n\n\nSo clearly the model is not going to converge. But maybe this is just a fluke. Let’s try again with a smaller learning rate. We will set the learning rate to 0.1. This is a reasonable value for a learning rate. Observe how the loss history of our model converges to a solution over the course of 2000 epochs.\n\n\nShow the code\nLR2 = LogisticRegression()\nLR2.fit(X_dnc, y_dnc, alpha = 0.1, epochs = 2000)\n\nnum_steps = len(LR2.loss_history)\n\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient (alpha = 29)\")\nplt.plot(np.arange(num_steps) + 1, LR2.loss_history, label = \"gradient (alpha = 0.1)\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\n\nax= plt.gca()\n\nplt.show()\n\nNone # To suppress Matplotlib output\n\n\nFit model with gradient descent in 2000/2000(max) epochs.\n\n\n\n\n\n\n\nExample 2: Batch size & convergence speed\nWith stochastic gradient descent, we can control the convergence speed of our model by adjusting the batch size. Again, we will test this claim on the make_moons dataset from sklearn.datasets. This dataset is not linearly separable.\nUsing two models of stochastic gradient descent with momentum with differing batch sizes (20 and 100), we will show that the batch size controls the convergence speed of the model.\n\n\nShow the code\n# Batch size influencing convergence time\n\nseed = 12345\n\nLR20 = LogisticRegression(seed=seed)\nLR20.fit_stochastic(X, y, alpha = 0.05, epochs = 1000, momentum = True, batch_size = 20)\n\nLR100 = LogisticRegression(seed=seed)\nLR100.fit_stochastic(X, y, alpha = 0.05, epochs = 1000, momentum = True, batch_size = 100)\n\n\n# Plot loss history\nnum_steps = len(LR20.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR20.loss_history, label = \"batch size = 20\")\nplt.plot(np.arange(num_steps) + 1, LR100.loss_history, label = \"batch size = 100\")\n\nlegend = plt.legend()\n\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\n\nplt.loglog()\n\nax= plt.gca()\n\nplt.show()\n\n\nFit model with stochastic gradient descent in 1000/1000(max) epochs.\nFit model with stochastic gradient descent in 1000/1000(max) epochs.\n\n\n\n\n\nObserve how the model with a smaller batch size (20) converges to a solution much faster than the model with a larger batch size (100). This is because the model with a smaller batch size makes more updates per epoch than the model with a larger batch size.\nThis does not mean small sizes are always better. For example, if we use a batch size of 2, the model can bounce around our minimizer and not converge to a solution. This is because the model with a batch size of 2 makes worse average approximations to the gradient. And such a small sample size would be very noisy.\nObserve that the model with a batch size of 2 leads to a noisy loss history.\n\n\nShow the code\n# Batch size influencing convergence time\n\nseed = 12345\n\nLR20 = LogisticRegression(seed=seed)\nLR20.fit_stochastic(X, y, alpha = 0.05, epochs = 1000, momentum = True, batch_size = 20)\n\nLR1 = LogisticRegression(seed=seed)\nLR1.fit_stochastic(X, y, alpha = 0.05, epochs = 1000, momentum = True, batch_size = 2)\n\n\n# Plot loss history\nnum_steps = len(LR20.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR20.loss_history, label = \"batch size = 20\")\nplt.plot(np.arange(num_steps) + 1, LR1.loss_history, label = \"batch size = 2\")\n\nlegend = plt.legend()\n\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\n\nplt.loglog()\n\nax= plt.gca()\n\nplt.show()\n\n\nFit model with stochastic gradient descent in 1000/1000(max) epochs.\nFit model with stochastic gradient descent in 1000/1000(max) epochs.\n\n\n\n\n\n\n\nExample 3: SGD with momentum speeding up convergence on high dimensional data\nWe will show that stochastic gradient descent with momentum can speed up convergence on high dimensional data. We will use the make_blobs dataset from sklearn.datasets to generate a dataset with 10 features and 2 classes. We use a large sample size of 40,000 observations.\n\n\nShow the code\n# Example: momentum leading to faster convergence\nseed = 12345\nepochs = 1000\nalpha = 0.01\n\nnp.random.seed(seed)\nn_features = 10\ncenters = [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\nX_m, y_m = make_blobs(n_samples=40000, n_features=n_features, centers=centers, random_state=seed, cluster_std=1.8)\n\nLR = LogisticRegression(seed)\nLR.fit(X_m, y_m, alpha = alpha, epochs = epochs)\n\nLRS = LogisticRegression(seed)\nLRS.fit_stochastic(X_m, y_m, alpha = alpha, epochs = epochs, momentum = False, batch_size = 2000)\n\nLRM = LogisticRegression(seed)\nLRM.fit_stochastic(X_m, y_m, alpha = alpha, epochs = epochs, momentum = True, batch_size = 2000)\n\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"GD\")\nplt.plot(np.arange(num_steps) + 1, LRS.loss_history, label = \"SGD, no momentum\")\nplt.plot(np.arange(num_steps) + 1, LRM.loss_history, label = \"SGD, momentum\")\n\nlegend = plt.legend()\n\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\n\nplt.loglog()\n\nax= plt.gca()\n\nplt.show()\n\n\nFit model with gradient descent in 1000/1000(max) epochs.\nFit model with stochastic gradient descent in 1000/1000(max) epochs.\nFit model with stochastic gradient descent in 1000/1000(max) epochs.\n\n\n\n\n\nHere we observe the significant speedup offered by stochastic gradient descent with momentum. The stochastic variant with momentum converges to a solution in ~100 epochs, while the stochastic variant without momentum converges to a solution in ~1000 epochs. And the normal variant converges to a solution in >10,000 epochs.\n\n\nExample 4: Large epsilon leading to early exit\nConsider the following example where we set the epsilon threshold to a large value (0.01).\n\n\nShow the code\n# Example: large epsilon leading to early exit\n\nseed = 12345\nepochs = 1000\nalpha = 0.05\nepilson = 0.01\n\nkwargs = {\"seed\": seed, \"epsilon\": epilson}\nLR = LogisticRegression(**kwargs)\n\nLR.fit(X, y, alpha = alpha, epochs = epochs)\n\n# LR.score(X, y)\n\n\nFit model with gradient descent in 13/1000(max) epochs.\n\n\nObserve how the model exiting in 13 epochs. If \\(\\epsilon\\) is too large, the model will exit before it has converged to the actual minimizer. This is bad. In general, we want to set \\(\\epsilon\\) to a small value."
  },
  {
    "objectID": "posts/logistic-regression-and-gradient-descent/index.html#sources",
    "href": "posts/logistic-regression-and-gradient-descent/index.html#sources",
    "title": "Logistic Regression & Gradient Descent",
    "section": "Sources:",
    "text": "Sources:\n\n[1] Sigmoid Image\n[2] Alpha Overshoot Image"
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "source: perceptron.py\nWelcome to the first in a series of blog posts exploring the fundamentals of machine learning. This blog is a component of CSCI 0451: Machine Learning at Middlebury College. My name is Nicholas Sliter, and I am a senior at Middlebury College studying Computer Science and Mathematics.\nIn this post, we will be exploring the perceptron algorithm, a simple machine learning algorithm that is the basis for many more complex algorithms. We will be implementing the perceptron algorithm in Python and using it classify binary-labeled synthetic data.\nTo begin, we will generate a linearly separable dataset of binary-labeled 2D points.\nWe will use the make_blobs function from sklearn.datasets to generate the data. The make_blobs function takes in a number of samples, a number of features, and a number of classes. It then generates a dataset of points with the specified number of samples and features, and labels the points with the specified number of classes.\nWe can see two distinct clusters of points. We also observe that the points are linearly separable, meaning that we can draw a line that separates the two clusters of points."
  },
  {
    "objectID": "posts/perceptron/index.html#perceptron-model",
    "href": "posts/perceptron/index.html#perceptron-model",
    "title": "Perceptron",
    "section": "Perceptron Model",
    "text": "Perceptron Model\nNow we will find that separating line using a custom-made perceptron model. The model takes in our feature matrix X and our label vector y. It then performs the following steps:\n\nInitialize the weights and bias to random values.\nIterate through the data, updating the weights and bias until…\n\nthe model is able to correctly classify all of the points, or\nthe maximum number of iterations is reached, or\nthe model reaches an optionally-defined loss threshold.\n\n\nThe fit method performs the steps above. The predict method takes in a feature matrix X and returns the predicted labels for each point in X.\nAnd perceptron.history gives a list of accuracy scores for each iteration of the model.\n\nfrom perceptron import Perceptron\n\nperceptron = Perceptron()\n\nperceptron.fit(X, y) # type: ignore\n\nfig = plt.plot(perceptron.history)\n\nFit model in 114 steps with score 1.0\n\n\n\n\n\nObserve the accuracy of the model need not follow a monotonic trend.\n\nperceptron.w\n\narray([ 0.68974191,  2.39236714, -0.80889108])\n\n\nHere we can see the separating line that the model found with the added bias term.\n\nprint(X[1])\n\n[2.06270619 2.55152881]\n\n\n\n# Test prediction on a fake point\n\npoint = np.array([[1, 2]]) # should have label 1\nprediction = perceptron.predict(point)\nprint(f\"Prediction for point {point} is {prediction[0]}\")\n\nPrediction for point [[1 2]] is 1\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"#333\", linestyle = \"dashed\")\n\n\ndef plot_history(history, step_size = 1):\n\n    ax = (sns.lineplot(\n            x = range(0, len(history)*step_size, step_size), \n            y = history, \n            errorbar=None,\n         ))\n\n    ax.set_ylabel(\"Accuracy\")\n    ax.set_xlabel(\"Step\")\n    ax.set_title(\"Perceptron Accuracy History\")\n    ax.set_facecolor(\"#fafafa\")\n\n    return ax\n    \n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y) # type: ignore\nfig = draw_line(perceptron.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nObserve that this line is precisely the separating line specified by perceptron.w.\n\ndef plot_simple_scatter(X, y, xlab = \"\", ylab = \"\", title = None,):\n    df = pl.DataFrame({\"x\": X[:,0], \"y\": X[:,1], \"label\": y})\n    ax = (sns.scatterplot(\n        data = df,\n        x = \"x\",\n        y = \"y\",\n        hue = \"label\",\n        palette = [\"g\", \"b\"],\n    ))\n\n    ax.set_xlabel(xlab)\n    ax.set_ylabel(ylab)\n\n    # ax.set_facecolor(\"#fafafa\")\n    ax.set_facecolor(\"#f1f3f5\")\n\n    if title is not None:\n        ax.set_title(title)\n\n    return ax # allows for further customization\n\n\n\nWe know that the perceptron model is guaranteed to converge to a separating line if the data is linearly separable. But what happens if we feed the model data that is not linearly separable?"
  },
  {
    "objectID": "posts/perceptron/index.html#non-linearly-separable-data",
    "href": "posts/perceptron/index.html#non-linearly-separable-data",
    "title": "Perceptron",
    "section": "Non-Linearly Separable Data:",
    "text": "Non-Linearly Separable Data:\n\n# Let's try to fit to a non-linearly separable dataset\n\nseed = 12345 # we are reseeding so we can run each cell independently\nnp.random.seed(seed)\n\nnX, ny = make_blobs(n_samples = 300, n_features = p_features - 1, centers = [(1.7, 0.5), (1.7, 1.7)], cluster_std = 0.5) # type: ignore\n\nax = plot_simple_scatter(nX, ny, xlab = \"Feature 1\", ylab = \"Feature 2\", title = \"Non-linearly separable dataset\")\n\n\n\n\nClearly, this data is not linearly separable. Let’s see what happens when we feed it to the perceptron model.\n\n# Let's try to fit to a non-linearly separable dataset\np = Perceptron()\np.fit(nX, ny, 10000) # type: ignore\n\nFit model in 10000 steps with score 0.8866666666666667\n\n\nSo we converged after max_steps iterations. But we did not converge to a line that perfectly separates the data. Indeed, under these conditions, if we did not feed the model a maximum number of iterations, it would never converge and would instead run forever.\n\n# plot_history(p.history) produces too much noise\nhistory_subset = [p.history[i] for i in range(0, len(p.history), 100)]\n\nax = plot_history(history_subset, 100)\n\n\n\n\nAs you can see the accuracy of the model ‘ping-pongs’ around and is never able to reach far above 0.9. But what if we had a way to short circuit the model when it’s accuracy is ‘good enough’?\nwhile (not self.__has_epsilon_converged(X_, y, epsilon)) and i < max_steps:\n            self.__update_weights(X_, y, self.w, rate)\n            self.__record_history(X_, y)\n            i += 1\nLooking into our loop code, this is exactly what __has_epsilon_converged(X_, y, epsilon) does. Given some \\(\\epsilon\\)-threshold, it checks if the model’s loss is \\(\\leq\\) that threshold. If it is, it returns True and the model stops training. If it is not, it returns False and the model continues training.\nBut this example was trained with \\(\\epsilon = 0\\), meaning it only stops when it reaches 100% accuracy.\n\n# Check accuracy and plot decision boundary\np.score(nX, ny) # type: ignore\n\nax = plot_simple_scatter(nX, ny, xlab = \"Feature 1\", ylab = \"Feature 2\", title = \"Non-linearly separable dataset\")\nax = draw_line(p.w, 0, 4)\n\n\n\n\nLet’s see if we can improve our model by using this new feature.\n\n# Now let's try with epsilon = 0.15\n\nseed = 12345\nnp.random.seed(seed)\n\np = Perceptron()\nlr = 1\ne = 0.15\np.fit(nX, ny, 10000, lr, e) # type: ignore\n\nax = plot_simple_scatter(nX, ny, xlab = \"Feature 1\", ylab = \"Feature 2\", title = \"Non-linearly separable dataset\")\nax = draw_line(p.w, 0, 4)\n\nFit model in 9 steps with score 0.8866666666666667\n\n\n\n\n\nModel short-circuiting works. In 9 steps, we have higher accuracy than the 10k step previous example. With some hyperparameter tuning on \\(\\epsilon\\), we could probably get even better results."
  },
  {
    "objectID": "posts/perceptron/index.html#higher-dimensional-data",
    "href": "posts/perceptron/index.html#higher-dimensional-data",
    "title": "Perceptron",
    "section": "Higher Dimensional Data:",
    "text": "Higher Dimensional Data:\nWe can also use the perceptron model to classify data with more than two features. Let’s see what happens when we feed the model data with five features.\nWhile we lose the ability to (easily) visualize the data, we will still see that the model is able to find a separating line.\n\n# Try higher dimensionality (5 features)\n\nseed = 12345\nnp.random.seed(seed)\n\ncenter1 = np.random.uniform(-2, 2, 5)\ncenter2 = np.random.uniform(-2, 2, 5)\n\nhX, hy = make_blobs(n_samples = 300, n_features = 5, centers = [center1, center2], cluster_std = 0.5) # type: ignore\n\n# Test f1 x f2\nax = plot_simple_scatter(hX[:,[0,1]], hy, xlab = \"Feature 1\", ylab = \"Feature 2\", title = \"Five-dimensional dataset (f1 x f2)\")\n\n\n\n\nThe above (f1 x f2) plot is a 2D projection of the 5D data. This plot is not very useful, but it provides a limited visual representation of the data and interestingly, implies that the data is linearly separable.\n\nseed = 123456 # seed of 12345 converges in 1 step\nnp.random.seed(seed)\n\np = Perceptron()\np.fit(hX, hy, 10000, 1) # type: ignore\n\nFit model in 33 steps with score 1.0\n\n\n\nax = plot_history(p.history)\n\n\n\n\nClearly, our 5D data is linearly separable since we achieve 100% accuracy."
  },
  {
    "objectID": "posts/perceptron/index.html#update-step-time-complexity",
    "href": "posts/perceptron/index.html#update-step-time-complexity",
    "title": "Perceptron",
    "section": "Update-step time complexity:",
    "text": "Update-step time complexity:\n\\[\\tilde{\\mathbf{w}}^{(t+1)} = \\tilde{\\mathbf{w}}^{(t)} + \\mathbb{1}(\\tilde{y}_i \\langle \\tilde{\\mathbf{w}}^{(t)}, \\tilde{\\mathbf{x}}_i\\rangle < 0)\\tilde{y}_i \\tilde{\\mathbf{x}}_i\\;\\]\nThe update step features a dot product between the weights and the input vector. A dot product is O(p) where p is the number of features. The other operations are simple additions and multiplications, which are ~O(1). The dominate operations are O(p), therefore, the update step has an overall time complexity of O(p).\n\nConsider the following weight update function:\n\n    def __update_weights(self, X, y, w, rate) -> None:\n        ''' Internal method to update model weights\n        @param X: 2D array of shape (n_samples, n_features + 1)\n        @param y: 1D array of shape (n_samples) of labels (0 or 1)\n        @param w: 1D array of shape (n_features + 1) of weights\n        @param rate: float of the learning rate\n        @return: None\n        '''\n        index = np.random.randint(0, len(X))\n        y_hat = 2 * y[index] - 1\n\n        self.w = w + int(y_hat * np.dot(X[index], w) < 0) * rate * (X[index] * y_hat)\nThis is the in-code implementation of the update step. We begin by randomly selecting a point from the dataset. We get the label of this point and convert it to a \\(-1\\) or \\(1\\). We then compute the dot product between the weights and the input vector. If the dot product is less than \\(0\\), we update the weights.\nThe updated weight vector is computed by multiplying the learning rate (\\(1\\)) by the input vector and our modified label \\(\\in \\{-1,1\\}\\). We then add this to the existing weights.\nThe only computationally expensive operation is the dot product. All others, including the random index selection, array indexing, type coercion, addition, and multiplication are ~O(1)."
  },
  {
    "objectID": "posts/perceptron/index.html#sources",
    "href": "posts/perceptron/index.html#sources",
    "title": "Perceptron",
    "section": "Sources:",
    "text": "Sources:\n\n[1] Perceptron title-card image"
  }
]