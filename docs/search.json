[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS451 Course Blog",
    "section": "",
    "text": "Exploring a basic Perceptron algorithm in Python.\n\n\n\n\n\n\nFeb 26, 2023\n\n\nNicholas Sliter\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "source: perceptron.py\nTo begin, we will generate a linearly separable dataset of binary-labeled 2D points.\nWe will use the make_blobs function from sklearn.datasets to generate the data. The make_blobs function takes in a number of samples, a number of features, and a number of classes. It then generates a dataset of points with the specified number of samples and features, and labels the points with the specified number of classes.\nWe can see two distinct clusters of points. We also observe that the points are linearly separable, meaning that we can draw a line that separates the two clusters of points."
  },
  {
    "objectID": "posts/perceptron/index.html#sources",
    "href": "posts/perceptron/index.html#sources",
    "title": "Perceptron",
    "section": "Sources:",
    "text": "Sources:\n\n[1] Perceptron title-card image"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/perceptron/index.html#write-up",
    "href": "posts/perceptron/index.html#write-up",
    "title": "Perceptron",
    "section": "Write-up:",
    "text": "Write-up:"
  },
  {
    "objectID": "posts/perceptron/index.html#update-step-time-complexity",
    "href": "posts/perceptron/index.html#update-step-time-complexity",
    "title": "Perceptron",
    "section": "Update-step time complexity:",
    "text": "Update-step time complexity:\n\\[\\tilde{\\mathbf{w}}^{(t+1)} = \\tilde{\\mathbf{w}}^{(t)} + \\mathbb{1}(\\tilde{y}_i \\langle \\tilde{\\mathbf{w}}^{(t)}, \\tilde{\\mathbf{x}}_i\\rangle < 0)\\tilde{y}_i \\tilde{\\mathbf{x}}_i\\;\\]\nThe update step features a dot product between the weights and the input vector. A dot product is O(n) where n is the number of features. The other operations are simple additions and multiplications, which are ~O(1). Therefore, the update step has a time complexity of O(n).\n\nConsider the following weight update function:\n\n    def __update_weights(self, X, y, w, rate) -> None:\n        ''' Internal method to update model weights\n        @param X: 2D array of shape (n_samples, n_features + 1)\n        @param y: 1D array of shape (n_samples) of labels (0 or 1)\n        @param w: 1D array of shape (n_features + 1) of weights\n        @param rate: float of the learning rate\n        @return: None\n        '''\n        index = np.random.randint(0, len(X))\n        y_hat = 2 * y[index] - 1\n\n        self.w = w + int(y_hat * np.dot(X[index], w) < 0) * rate * (X[index] * y_hat)\nThis is the in-code implementation of the update step. We begin by randomly selecting a point from the dataset. We get the label of this point and convert it to a \\(-1\\) or \\(1\\). We then compute the dot product between the weights and the input vector. If the dot product is less than \\(0\\), we update the weights.\nThe updated weight vector is computed by multiplying the learning rate (\\(1\\)) by the input vector and our modified label \\(\\in \\{-1,1\\}\\). We then add this to the existing weights.\nThe only computationally expensive operation is the dot product. All others, including the random index selection, array indexing, type coercion, addition, and multiplication are ~O(1)."
  },
  {
    "objectID": "posts/perceptron/index.html#perceptron-model",
    "href": "posts/perceptron/index.html#perceptron-model",
    "title": "Perceptron",
    "section": "Perceptron Model",
    "text": "Perceptron Model\nNow we will find that separating line using a custom-made perceptron model. The model takes in our feature matrix X and our label vector y. It then performs the following steps:\n\nInitialize the weights and bias to random values.\nIterate through the data, updating the weights and bias until…\n\nthe model is able to correctly classify all of the points, or\nthe maximum number of iterations is reached, or\nthe model reaches an optionally-defined loss threshold.\n\n\nThe fit method performs the steps above. The predict method takes in a feature matrix X and returns the predicted labels for each point in X.\nAnd perceptron.history gives a list of accuracy scores for each iteration of the model.\n\nfrom perceptron import Perceptron\n\nperceptron = Perceptron()\n\nperceptron.fit(X, y) # type: ignore\n\nfig = plt.plot(perceptron.history)\n\nFit model in 114 steps with score 1.0\n\n\n\n\n\nObserve the accuracy of the model need not follow a monotonic trend.\n\nperceptron.w\n\narray([ 0.68974191,  2.39236714, -0.80889108])\n\n\nHere we can see the separating line that the model found with the added bias term.\n\nprint(X[1])\n\n[2.06270619 2.55152881]\n\n\n\n# Test prediction on a fake point\n\npoint = np.array([[1, 2]]) # should have label 1\nprediction = perceptron.predict(point)\nprint(f\"Prediction for point {point} is {prediction[0]}\")\n\nPrediction for point [[1 2]] is 1\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"#333\", linestyle = \"dashed\")\n\n\ndef plot_history(history, step_size = 1):\n\n    ax = (sns.lineplot(\n            x = range(0, len(history)*step_size, step_size), \n            y = history, \n            errorbar=None,\n         ))\n\n    ax.set_ylabel(\"Accuracy\")\n    ax.set_xlabel(\"Step\")\n    ax.set_title(\"Perceptron Accuracy History\")\n    ax.set_facecolor(\"#fafafa\")\n\n    return ax\n    \n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y) # type: ignore\nfig = draw_line(perceptron.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nObserve that this line is precisely the separating line specified by perceptron.w.\n\ndef plot_simple_scatter(X, y, xlab = \"\", ylab = \"\", title = None,):\n    df = pl.DataFrame({\"x\": X[:,0], \"y\": X[:,1], \"label\": y})\n    ax = (sns.scatterplot(\n        data = df,\n        x = \"x\",\n        y = \"y\",\n        hue = \"label\",\n        palette = [\"g\", \"b\"],\n    ))\n\n    ax.set_xlabel(xlab)\n    ax.set_ylabel(ylab)\n\n    # ax.set_facecolor(\"#fafafa\")\n    ax.set_facecolor(\"#f1f3f5\")\n\n    if title is not None:\n        ax.set_title(title)\n\n    return ax # allows for further customization\n\n\n\nWe know that the perceptron model is guaranteed to converge to a separating line if the data is linearly separable. But what happens if we feed the model data that is not linearly separable?"
  },
  {
    "objectID": "posts/perceptron/index.html#higher-dimensional-data",
    "href": "posts/perceptron/index.html#higher-dimensional-data",
    "title": "Perceptron",
    "section": "Higher Dimensional Data:",
    "text": "Higher Dimensional Data:\nWe can also use the perceptron model to classify data with more than two features. Let’s see what happens when we feed the model data with five features.\nWhile we lose the ability to (easily) visualize the data, we will still see that the model is able to find a separating line.\n\n# Try higher dimensionality (5 features)\n\nseed = 12345\nnp.random.seed(seed)\n\ncenter1 = np.random.uniform(-2, 2, 5)\ncenter2 = np.random.uniform(-2, 2, 5)\n\nhX, hy = make_blobs(n_samples = 300, n_features = 5, centers = [center1, center2], cluster_std = 0.5) # type: ignore\n\n# Test f1 x f2\nax = plot_simple_scatter(hX[:,[0,1]], hy, xlab = \"Feature 1\", ylab = \"Feature 2\", title = \"Five-dimensional dataset (f1 x f2)\")\n\n\n\n\nThe above (f1 x f2) plot is a 2D projection of the 5D data. This plot is not very useful, but it provides a limited visual representation of the data.\n\nseed = 123456 # seed of 12345 converges in 1 step\nnp.random.seed(seed)\n\np = Perceptron()\np.fit(hX, hy, 10000, 1) # type: ignore\n\nFit model in 33 steps with score 1.0\n\n\n\nax = plot_history(p.history)\n\n\n\n\nClearly, our 5D data is linearly separable since we achieve 100% accuracy."
  },
  {
    "objectID": "posts/perceptron/index.html#non-linearly-separable-data",
    "href": "posts/perceptron/index.html#non-linearly-separable-data",
    "title": "Perceptron",
    "section": "Non-Linearly Separable Data:",
    "text": "Non-Linearly Separable Data:\n\n# Let's try to fit to a non-linearly separable dataset\n\nseed = 12345 # we are reseeding so we can run each cell independently\nnp.random.seed(seed)\n\nnX, ny = make_blobs(n_samples = 300, n_features = p_features - 1, centers = [(1.7, 0.5), (1.7, 1.7)], cluster_std = 0.5) # type: ignore\n\nax = plot_simple_scatter(nX, ny, xlab = \"Feature 1\", ylab = \"Feature 2\", title = \"Non-linearly separable dataset\")\n\n\n\n\nClearly, this data is not linearly separable. Let’s see what happens when we feed it to the perceptron model.\n\n# Let's try to fit to a non-linearly separable dataset\np = Perceptron()\np.fit(nX, ny, 10000) # type: ignore\n\nFit model in 10000 steps with score 0.8866666666666667\n\n\nSo we converged after max_steps iterations. But we did not converge to a line that perfectly separates the data. Indeed, under these conditions, if we did not feed the model a maximum number of iterations, it would never converge and would instead run forever.\n\n# plot_history(p.history) produces too much noise\nhistory_subset = [p.history[i] for i in range(0, len(p.history), 100)]\n\nax = plot_history(history_subset, 100)\n\n\n\n\nAs you can see the accuracy of the model ‘ping-pongs’ around and is never able to reach far above 0.9. But what if we had a way to short circuit the model when it’s accuracy is ‘good enough’?\nwhile (not self.__has_epsilon_converged(X_, y, epsilon)) and i < max_steps:\n            self.__update_weights(X_, y, self.w, rate)\n            self.__record_history(X_, y)\n            i += 1\nLooking into our loop code, this is exactly what __has_epsilon_converged(X_, y, epsilon) does. Given some \\(\\epsilon\\)-threshold, it checks if the model’s loss is \\(\\leq\\) that threshold. If it is, it returns True and the model stops training. If it is not, it returns False and the model continues training.\nBut this example was trained with \\(\\epsilon = 0\\), meaning it only stops when it reaches 100% accuracy.\n\n# Check accuracy and plot decision boundary\np.score(nX, ny) # type: ignore\n\nax = plot_simple_scatter(nX, ny, xlab = \"Feature 1\", ylab = \"Feature 2\", title = \"Non-linearly separable dataset\")\nax = draw_line(p.w, 0, 4)\n\n\n\n\nLet’s see if we can improve our model by using this new feature.\n\n# Now let's try with epsilon = 0.15\n\nseed = 12345\nnp.random.seed(seed)\n\np = Perceptron()\nlr = 1\ne = 0.15\np.fit(nX, ny, 10000, lr, e) # type: ignore\n\nax = plot_simple_scatter(nX, ny, xlab = \"Feature 1\", ylab = \"Feature 2\", title = \"Non-linearly separable dataset\")\nax = draw_line(p.w, 0, 4)\n\nFit model in 9 steps with score 0.8866666666666667\n\n\n\n\n\nModel short-circuiting works. In 9 steps, we have higher accuracy than the 10k step previous example. With some hyperparameter tuning on \\(\\epsilon\\), we could probably get even better results."
  }
]