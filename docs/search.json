[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS451 Course Blog",
    "section": "",
    "text": "Exploring unsupervised learning throigh SVD and Spectral Clustering\n\n\n\n\n\n\nApr 15, 2023\n\n\nNicholas Sliter\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nExploring kernel logistic regression\n\n\n\n\n\n\nApr 11, 2023\n\n\nNicholas Sliter\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nExploring logistic regression and gradient descent\n\n\n\n\n\n\nMar 27, 2023\n\n\nNicholas Sliter\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nExploring a basic Perceptron algorithm in Python.\n\n\n\n\n\n\nFeb 26, 2023\n\n\nNicholas Sliter\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/kernel-logistic-regression/index.html",
    "href": "posts/kernel-logistic-regression/index.html",
    "title": "Kernel Logistic Regression",
    "section": "",
    "text": "Source code for this post can be found here: KernelLogisticRegression.py\nRE: my last post on Logistic Regression, what happens if our data is very non-linear?\nConsider the following example:\nObviously, a linear decision boundary in \\(\\mathbb{R}^2\\) will not work here. This motivates todays post on Kernel Logistic Regression.\nIn Kernel Logistic Regression, we use the kernel trick to transform our data into a higher dimensional space. Then we do classification in that high dimensional space. This is useful when we have data that is not linearly separable in the original space. We can use the kernel trick to transform our data into a higher dimensional space where it may be linearly separable.\nThe kernel is in essence a way for us to map our data to a higher dimensional space. Once that mapping has been done, the model is very similar to the logistic regression models we have seen before."
  },
  {
    "objectID": "posts/kernel-logistic-regression/index.html#empirical-risk",
    "href": "posts/kernel-logistic-regression/index.html#empirical-risk",
    "title": "Kernel Logistic Regression",
    "section": "Empirical Risk",
    "text": "Empirical Risk\nIn Kernel Logistic Regression, we change how we make predictions. Instead of the familiar \\(w \\cdot X\\) we saw in logistic regression, we will be using a vector of kernel weights \\(v\\) and a kernel matrix \\(K\\). The kernel matrix is computed by applying the kernel function to the points in our training data.\nFollowing this modification, we calculate the empirical risk of the model by computing the kernel matrix using the training data. We then use the kernel matrix to compute our predictions by multiplying our vector of kernel weights by the kernel matrix. We then compute the logistic loss between our predictions and the true labels. This gives us the empirical risk of the model.\ndef __emphirical_risk(self, X_, y, v) -> float:\n    km = self.kernel(self.X_train, X_, **self.kernel_kwargs)\n    y_pred = v @ km\n    return self.logistic_loss(y, y_pred)"
  },
  {
    "objectID": "posts/kernel-logistic-regression/index.html#examples",
    "href": "posts/kernel-logistic-regression/index.html#examples",
    "title": "Kernel Logistic Regression",
    "section": "Examples",
    "text": "Examples\nLet’s consider the following simple example using the make_moons dataset from sklearn.datasets.\n\nfrom KernelLogisticRegression import KernelLogisticRegression\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(100, shuffle = True, noise = 0.1)\nKLR = KernelLogisticRegression(rbf_kernel, gamma = .1)\nKLR.fit(X, y)\nprint(KLR.score(X, y))\n\n0.99\n\n\n\nX, y = make_moons(200, shuffle = True, noise = 0.1)\nplot_decision_regions(X, y, clf = KLR)\ntitle = plt.gca().set(title = f\"Accuracy = {KLR.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nObserve how the plotted decision boundary is non-linear but separates the data almost perfectly. Again, this is because the data was transformed into a higher dimensional space where our decision boundary is linear."
  },
  {
    "objectID": "posts/kernel-logistic-regression/index.html#choosing-gamma",
    "href": "posts/kernel-logistic-regression/index.html#choosing-gamma",
    "title": "Kernel Logistic Regression",
    "section": "Choosing Gamma",
    "text": "Choosing Gamma\nConsider the hyperparameter gamma. In essence, gamma controls how complex we allow our decision boundary to be. If gamma is too small, our decision boundary will be too simple. If gamma is too large, our decision boundary will be too complex. This is illustrated in the following example:\n\nX, y = make_moons(100, shuffle = True, noise = 0.15)\nKLRG = KernelLogisticRegression(rbf_kernel, gamma = 200000)\nKLRG.fit(X, y)\nprint(KLRG.score(X, y))\n\n1.0\n\n\nWow! With \\(gamma = 200000\\) we achieve perfect classification on the training data. This is because we have allowed our decision boundary to be very complex. But is this a good thing?\n\n\n\n\n\nObserve how we have created small circles around our orange points. This decision boundary is too complex. It will not generalize.\nLet’s generate new data from the same distribution and see how our model performs on it.\n\nX, y = make_moons(300, shuffle = True, noise = 0.15)\nplot_decision_regions(X, y, clf = KLRG)\ntitle = plt.gca().set(title = f\"Accuracy = {KLRG.score(X, y)}\",\n                      xlabel = \"Feature 1\", \n                      ylabel = \"Feature 2\")\n\n\n\n\nAs you can see, our model does not generalize well. This is because our choice of gamma was too large and caused overfitting. Gamma must be chosen carefully."
  },
  {
    "objectID": "posts/kernel-logistic-regression/index.html#vary-the-noise",
    "href": "posts/kernel-logistic-regression/index.html#vary-the-noise",
    "title": "Kernel Logistic Regression",
    "section": "Vary the Noise",
    "text": "Vary the Noise\nNow let’s vary the noise in the data. We will examine how noise and gamma interact to affect the performance of our model.\n\ndef gamma_noise_experiment(gamma_list, noise):\n    res = []\n    for gamma in gamma_list:\n        X, y = make_moons(60, shuffle = True, noise = noise)\n        KLR = KernelLogisticRegression(rbf_kernel, gamma = gamma)\n        KLR.fit(X, y)\n        X_test, y_test = make_moons(200, shuffle = True, noise = noise)\n        res.append({\n            \"noise\": noise,\n            \"gamma\": gamma,\n            \"train_accuracy\": KLR.score(X, y),\n            \"test_accuracy\": KLR.score(X_test, y_test)\n        })\n    return res\n\n\ngamma_list = [0.0001, 0.001, 0.1, 0.2, 0.5, 1, 5, 10, 100]\nnoise_list = [0, 0.1, 0.2, 0.4, 0.8, 1]\n\n\n\nShow the code\ncolumns = [\"noise\", \"gamma\", \"train_accuracy\", \"test_accuracy\"]\nresults = []\n\nfor noise in noise_list:\n    res = gamma_noise_experiment(gamma_list, noise)\n    results.append(res)\n\n\n\n\n\n\n\nObserve that as our noise increases, our train and test accuracy decrease. Observe also that as gamma increases our training accuracy approaches 100%. But clearly this causes overfit as we observe the test accuracy tends to decrease as gamma increases."
  },
  {
    "objectID": "posts/kernel-logistic-regression/index.html#circles",
    "href": "posts/kernel-logistic-regression/index.html#circles",
    "title": "Kernel Logistic Regression",
    "section": "Circles",
    "text": "Circles\nNow let’s circle back to our initial example and examine how our model performs on the make_circles dataset.\n\nfrom sklearn.datasets import make_circles\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_circles(200, shuffle = True, noise = 0.02, factor = 0.5)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\nKLR = KernelLogisticRegression(rbf_kernel, gamma = 1)\nKLR.fit(X_train, y_train)\n\nprint(f\"Train score: {KLR.score(X_train, y_train)}\")\nprint(f\"Test score: {KLR.score(X_test, y_test)}\")\n\n\n\nTrain score: 1.0\nTest score: 1.0\n\n\n\nplot_decision_regions(X_train, y_train, clf = KLR)\n\n<AxesSubplot: >\n\n\n\n\n\nAs you can see, kernel logistic regression is able to learn a non-linear decision boundary that separates the data almost perfectly."
  },
  {
    "objectID": "posts/logistic-regression-and-gradient-descent/index.html",
    "href": "posts/logistic-regression-and-gradient-descent/index.html",
    "title": "Logistic Regression & Gradient Descent",
    "section": "",
    "text": "Source LogisticRegression.py\nThis blog post outlines an implementation of linear classification via logistic regression with gradient descent, stochastic gradient descent, and stochastic gradient descent with momentum. I will address the implementations and differing convergence rates of each method.\nLogistic Regression is based around the sigmoid function, a convex function defined as: \\[\\sigma(x) = \\frac{1}{1 + e^{-x}}\\].\nUsing numpy, we define a vectorized sigmoid function as:"
  },
  {
    "objectID": "posts/logistic-regression-and-gradient-descent/index.html#learning-rate",
    "href": "posts/logistic-regression-and-gradient-descent/index.html#learning-rate",
    "title": "Logistic Regression & Gradient Descent",
    "section": "Learning Rate",
    "text": "Learning Rate\nNow naturally, we’d like our algorithm to converge to a global minimum. And as it stands, this is not guaranteed. We can however, make it more likely by using a learning rate \\(\\alpha \\in (0,1)\\) to control the step size of our gradient descent algorithm. The intuition behind this is that if we take a step in the direction of the gradient, we will move closer to the global minimum. But if we step too far, we may overshoot the global minimum and end up somewhere worse.\n\n\n\nWhen alpha is too large we can bounce around the global minimum and never reach it. stats.stackexchange.com\n\n\nBy making smaller steps, we decrease the likelihood of overshooting the global minimum. By sufficiently decreasing our step size, we can guarantee convergence at the cost of training time. Our new gradient descent algorithm is given as:\n\\[ w_{i+1} = w_{i} - \\alpha \\nabla L(w) \\]"
  },
  {
    "objectID": "posts/logistic-regression-and-gradient-descent/index.html#stochastic-gradient-descent",
    "href": "posts/logistic-regression-and-gradient-descent/index.html#stochastic-gradient-descent",
    "title": "Logistic Regression & Gradient Descent",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\nThis equation can be quite costly to compute, especially if we have a large number of samples. We can reduce the cost of this computation by using a batch \\(b\\) to compute the gradient of the loss function over a subset of the data. The intuition behind this is that the gradient of the loss function over a subset of the data is probably a good approximation of the gradient of the loss function over the entire dataset. And taking a step in the direction of this approximation should bring us closer to the global minimum.\nThis is known as stochastic gradient descent. The new algorithm is given as:\n\\[ w_{i+1} = w_{i} - \\alpha \\nabla L(w)_b \\]\nwhere \\(b\\) is a random subset of the data. Then we can iterate over the entire dataset by repeating this process for each batch."
  },
  {
    "objectID": "posts/logistic-regression-and-gradient-descent/index.html#stochastic-gradient-descent-with-momentum",
    "href": "posts/logistic-regression-and-gradient-descent/index.html#stochastic-gradient-descent-with-momentum",
    "title": "Logistic Regression & Gradient Descent",
    "section": "Stochastic Gradient Descent with Momentum",
    "text": "Stochastic Gradient Descent with Momentum\nThe problem with stochastic gradient descent is that it can be noisy. This is because we are taking a step in the direction of the gradient of the loss function over a random subset of the data. This can lead to the algorithm taking certain steps that are orthogonal or even opposite to the direction of the global minimum, and thus increase the noise at each step and increasing training time. We can reduce this noise by using momentum. The idea behind momentum is that we can keep track of the direction of the previous step and use this to help us take a step in the right direction. We can do this by adding a momentum term to our gradient descent algorithm, where \\(\\beta \\in (0,1)\\) is the momentum coefficient. This gives us the following equation:\n\\[ w_{i+1} = w_{i} - (\\alpha \\nabla L(w)_b + \\beta (w_{i} - w_{i-1})) \\]"
  },
  {
    "objectID": "posts/logistic-regression-and-gradient-descent/index.html#gradient-descent-implementations",
    "href": "posts/logistic-regression-and-gradient-descent/index.html#gradient-descent-implementations",
    "title": "Logistic Regression & Gradient Descent",
    "section": "Gradient Descent Implementations",
    "text": "Gradient Descent Implementations\nFor ease of implementation, gradient descent and its stochastic variants are implemented in two functions LogisticRegression.fit and LogisticRegression.fit_stochastic.\nLogisticRegression.fit_stochastic handles both stochastic gradient descent and stochastic gradient descent with momentum. As only difference between the two is the addition of the momentum term.\nWe define our model fit via gradient descent as:\ndef __fit(self, X, y, alpha, epochs) -> None:\n        ''' Internal method to fit the model\n        @param X: array of shape (n_samples, n_features + 1)\n        @param y: array of shape (n_samples,)\n        @param alpha: learning rate\n        @param epochs: number of epochs\n        @return: None\n        '''\n        if self.w is not None: raise Exception('Model is already trained')\n\n        _, n_features = X.shape\n        self.w = self.__get_random_weights(n_features)\n        self.__record_history(self.__loss(X, y), self.__score(X, y))\n\n        for epoch in range(epochs):\n        \n            y_pred = self.__predict(X)\n\n            # Update weights       \n            grad = self.__grad(X, y, y_pred)\n            update_amt  = (alpha * grad)\n            self.w -= update_amt\n\n            # Record loss and check for convergence\n            loss = self.__loss(X, y)\n            score = self.__score(X, y)\n\n            self.__record_history(loss, score)\n            if (np.linalg.norm(update_amt) <= self.epsilon): break\n        print(f\"Fit model with gradient descent in {epoch+1}/{epochs}(max) epochs.\")\nThe code for this follows from the above equation. We initialize our weights to random values, and then update them using the gradient of the loss function. We then record the loss and score of the model and check for convergence. If the model has converged, we break out of the loop. Otherwise, we continue to the next epoch.\nWe do this until convergence or until we reach the maximum number of epochs.\nSimilarly, we define our model fit via stochastic gradient descent as:\n\n def __fit_stochastic(self, X, y, alpha, epochs, batch_size, momentum=False, beta=0.8) -> None:\n        ''' Internal method to fit the model using stochastic gradient descent\n        @param X: array of shape (n_samples, n_features + 1)\n        @param y: array of shape (n_samples,)\n        @param alpha: learning rate\n        @param epochs: number of epochs\n        @param batch_size: size of batch\n        @param momentum: boolean to use momentum\n        @return: None\n        '''\n        beta: float = 1*(momentum*beta) # 0 when momentum is false\n\n        if self.w is not None: raise Exception('Model is already trained')\n\n        n, n_features = X.shape\n        self.w = self.__get_random_weights(n_features)\n        self.__record_history(self.__loss(X, y), self.__score(X, y))\n\n        for epoch in np.arange(epochs):\n            order = np.arange(n)\n            self.__rs.shuffle(order)\n\n            previous_update_vector = np.zeros(n_features)\n            for batch in np.array_split(order, n // batch_size + 1):\n                x_batch = X[batch,:]\n                y_batch = y[batch]\n                y_batch_pred = self.__predict(x_batch)\n                grad = self.__grad(x_batch, y_batch, y_batch_pred) \n\n                # Stochastic Gradient step with optional momentum\n                momentum_term =  (beta * (previous_update_vector))\n                update_vector = (alpha * grad) + momentum_term\n\n                previous_update_vector = update_vector\n\n                self.w -= update_vector\n\n            # Record loss and check for convergence\n            loss = self.__loss(X, y)\n            score = self.__score(X, y)\n\n            self.__record_history(loss, score)\n            if (np.linalg.norm(previous_update_vector) <= self.epsilon): break\n        print(f\"Fit model with stochastic gradient descent in {epoch+1}/{epochs}(max) epochs.\")\nHere we can see the subtle difference between gradient descent and its stochastic variants. We initialize our weights to random values, and then update them using the gradient of the loss function over a random subset of the data. We repeat until we have processed the entire dataset. Then, we repeat this process for the number of epochs. At the end of each epoch, we record the loss and score of the model and check for convergence. If the model has converged, we break out of the loop. Otherwise, we continue to the next epoch (and each batch within).\nWe define our momentum coefficient as beta = 0.8 which is a common convention."
  },
  {
    "objectID": "posts/logistic-regression-and-gradient-descent/index.html#examples",
    "href": "posts/logistic-regression-and-gradient-descent/index.html#examples",
    "title": "Logistic Regression & Gradient Descent",
    "section": "Examples:",
    "text": "Examples:\n\nExample 1: Large alpha leading to divergence\nConsider the following example where we set the learning rate to a large value. As earlier claimed, this can lead to divergence of the model.\nWe will test this claim on the make_moons dataset from sklearn.datasets. This dataset is not linearly separable.\n\n\nShow the code\n# Example: alpha too large\nfrom sklearn.datasets import make_moons\n\nX_dnc, y_dnc = make_moons(n_samples=400, noise=0.15, random_state=seed)\n\nfig = plt.scatter(X_dnc[:,0], X_dnc[:,1], c = y_dnc)\n\nNone # To suppress Matplotlib output\n\n\n\n\n\nWe will fit our model using normal gradient descent. We will set the learning rate to the obscenely large value of 29. Observe how the loss history of our model fails to converge to a solution over the course of 2000 epochs.\n\n\nShow the code\nLR = LogisticRegression()\nLR.fit(X_dnc, y_dnc, alpha = 29, epochs = 2000)\n\nnum_steps = len(LR.loss_history)\n\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\n\nax= plt.gca()\n\nplt.show()\n\nNone # To suppress Matplotlib output\n\n\nFit model with gradient descent in 2000/2000(max) epochs.\n\n\n\n\n\nSo clearly the model is not going to converge. But maybe this is just a fluke. Let’s try again with a smaller learning rate. We will set the learning rate to 0.1. This is a reasonable value for a learning rate. Observe how the loss history of our model converges to a solution over the course of 2000 epochs.\n\n\nShow the code\nLR2 = LogisticRegression()\nLR2.fit(X_dnc, y_dnc, alpha = 0.1, epochs = 2000)\n\nnum_steps = len(LR2.loss_history)\n\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient (alpha = 29)\")\nplt.plot(np.arange(num_steps) + 1, LR2.loss_history, label = \"gradient (alpha = 0.1)\")\n\nplt.loglog()\n\nlegend = plt.legend()\n\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\n\nax= plt.gca()\n\nplt.show()\n\nNone # To suppress Matplotlib output\n\n\nFit model with gradient descent in 2000/2000(max) epochs.\n\n\n\n\n\n\n\nExample 2: Batch size & convergence speed\nWith stochastic gradient descent, we can control the convergence speed of our model by adjusting the batch size. Again, we will test this claim on the make_moons dataset from sklearn.datasets. This dataset is not linearly separable.\nUsing two models of stochastic gradient descent with momentum with differing batch sizes (20 and 100), we will show that the batch size controls the convergence speed of the model.\n\n\nShow the code\n# Batch size influencing convergence time\n\nseed = 12345\n\nLR20 = LogisticRegression(seed=seed)\nLR20.fit_stochastic(X, y, alpha = 0.05, epochs = 1000, momentum = True, batch_size = 20)\n\nLR100 = LogisticRegression(seed=seed)\nLR100.fit_stochastic(X, y, alpha = 0.05, epochs = 1000, momentum = True, batch_size = 100)\n\n\n# Plot loss history\nnum_steps = len(LR20.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR20.loss_history, label = \"batch size = 20\")\nplt.plot(np.arange(num_steps) + 1, LR100.loss_history, label = \"batch size = 100\")\n\nlegend = plt.legend()\n\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\n\nplt.loglog()\n\nax= plt.gca()\n\nplt.show()\n\n\nFit model with stochastic gradient descent in 1000/1000(max) epochs.\nFit model with stochastic gradient descent in 1000/1000(max) epochs.\n\n\n\n\n\nObserve how the model with a smaller batch size (20) converges to a solution much faster than the model with a larger batch size (100). This is because the model with a smaller batch size makes more updates per epoch than the model with a larger batch size.\nThis does not mean small sizes are always better. For example, if we use a batch size of 2, the model can bounce around our minimizer and not converge to a solution. This is because the model with a batch size of 2 makes worse average approximations to the gradient. And such a small sample size would be very noisy.\nObserve that the model with a batch size of 2 leads to a noisy loss history.\n\n\nShow the code\n# Batch size influencing convergence time\n\nseed = 12345\n\nLR20 = LogisticRegression(seed=seed)\nLR20.fit_stochastic(X, y, alpha = 0.05, epochs = 1000, momentum = True, batch_size = 20)\n\nLR1 = LogisticRegression(seed=seed)\nLR1.fit_stochastic(X, y, alpha = 0.05, epochs = 1000, momentum = True, batch_size = 2)\n\n\n# Plot loss history\nnum_steps = len(LR20.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR20.loss_history, label = \"batch size = 20\")\nplt.plot(np.arange(num_steps) + 1, LR1.loss_history, label = \"batch size = 2\")\n\nlegend = plt.legend()\n\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\n\nplt.loglog()\n\nax= plt.gca()\n\nplt.show()\n\n\nFit model with stochastic gradient descent in 1000/1000(max) epochs.\nFit model with stochastic gradient descent in 1000/1000(max) epochs.\n\n\n\n\n\n\n\nExample 3: SGD with momentum speeding up convergence on high dimensional data\nWe will show that stochastic gradient descent with momentum can speed up convergence on high dimensional data. We will use the make_blobs dataset from sklearn.datasets to generate a dataset with 10 features and 2 classes. We use a large sample size of 40,000 observations.\n\n\nShow the code\n# Example: momentum leading to faster convergence\nseed = 12345\nepochs = 1000\nalpha = 0.01\n\nnp.random.seed(seed)\nn_features = 10\ncenters = [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]]\nX_m, y_m = make_blobs(n_samples=40000, n_features=n_features, centers=centers, random_state=seed, cluster_std=1.8)\n\nLR = LogisticRegression(seed)\nLR.fit(X_m, y_m, alpha = alpha, epochs = epochs)\n\nLRS = LogisticRegression(seed)\nLRS.fit_stochastic(X_m, y_m, alpha = alpha, epochs = epochs, momentum = False, batch_size = 2000)\n\nLRM = LogisticRegression(seed)\nLRM.fit_stochastic(X_m, y_m, alpha = alpha, epochs = epochs, momentum = True, batch_size = 2000)\n\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"GD\")\nplt.plot(np.arange(num_steps) + 1, LRS.loss_history, label = \"SGD, no momentum\")\nplt.plot(np.arange(num_steps) + 1, LRM.loss_history, label = \"SGD, momentum\")\n\nlegend = plt.legend()\n\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\n\nplt.loglog()\n\nax= plt.gca()\n\nplt.show()\n\n\nFit model with gradient descent in 1000/1000(max) epochs.\nFit model with stochastic gradient descent in 1000/1000(max) epochs.\nFit model with stochastic gradient descent in 1000/1000(max) epochs.\n\n\n\n\n\nHere we observe the significant speedup offered by stochastic gradient descent with momentum. The stochastic variant with momentum converges to a solution in ~100 epochs, while the stochastic variant without momentum converges to a solution in ~1000 epochs. And the normal variant converges to a solution in >10,000 epochs.\n\n\nExample 4: Large epsilon leading to early exit\nConsider the following example where we set the epsilon threshold to a large value (0.01).\n\n\nShow the code\n# Example: large epsilon leading to early exit\n\nseed = 12345\nepochs = 1000\nalpha = 0.05\nepilson = 0.01\n\nkwargs = {\"seed\": seed, \"epsilon\": epilson}\nLR = LogisticRegression(**kwargs)\n\nLR.fit(X, y, alpha = alpha, epochs = epochs)\n\n# LR.score(X, y)\n\n\nFit model with gradient descent in 13/1000(max) epochs.\n\n\nObserve how the model exiting in 13 epochs. If \\(\\epsilon\\) is too large, the model will exit before it has converged to the actual minimizer. This is bad. In general, we want to set \\(\\epsilon\\) to a small value."
  },
  {
    "objectID": "posts/logistic-regression-and-gradient-descent/index.html#sources",
    "href": "posts/logistic-regression-and-gradient-descent/index.html#sources",
    "title": "Logistic Regression & Gradient Descent",
    "section": "Sources:",
    "text": "Sources:\n\n[1] Sigmoid Image\n[2] Alpha Overshoot Image"
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "source: perceptron.py\nWelcome to the first in a series of blog posts exploring the fundamentals of machine learning. This blog is a component of CSCI 0451: Machine Learning at Middlebury College. My name is Nicholas Sliter, and I am a senior at Middlebury College studying Computer Science and Mathematics.\nIn this post, we will be exploring the perceptron algorithm, a simple machine learning algorithm that is the basis for many more complex algorithms. We will be implementing the perceptron algorithm in Python and using it classify binary-labeled synthetic data.\nTo begin, we will generate a linearly separable dataset of binary-labeled 2D points.\nWe will use the make_blobs function from sklearn.datasets to generate the data. The make_blobs function takes in a number of samples, a number of features, and a number of classes. It then generates a dataset of points with the specified number of samples and features, and labels the points with the specified number of classes.\nWe can see two distinct clusters of points. We also observe that the points are linearly separable, meaning that we can draw a line that separates the two clusters of points."
  },
  {
    "objectID": "posts/perceptron/index.html#perceptron-model",
    "href": "posts/perceptron/index.html#perceptron-model",
    "title": "Perceptron",
    "section": "Perceptron Model",
    "text": "Perceptron Model\nNow we will find that separating line using a custom-made perceptron model. The model takes in our feature matrix X and our label vector y. It then performs the following steps:\n\nInitialize the weights and bias to random values.\nIterate through the data, updating the weights and bias until…\n\nthe model is able to correctly classify all of the points, or\nthe maximum number of iterations is reached, or\nthe model reaches an optionally-defined loss threshold.\n\n\nThe fit method performs the steps above. The predict method takes in a feature matrix X and returns the predicted labels for each point in X.\nAnd perceptron.history gives a list of accuracy scores for each iteration of the model.\n\nfrom perceptron import Perceptron\n\nperceptron = Perceptron()\n\nperceptron.fit(X, y) # type: ignore\n\nfig = plt.plot(perceptron.history)\n\nFit model in 114 steps with score 1.0\n\n\n\n\n\nObserve the accuracy of the model need not follow a monotonic trend.\n\nperceptron.w\n\narray([ 0.68974191,  2.39236714, -0.80889108])\n\n\nHere we can see the separating line that the model found with the added bias term.\n\nprint(X[1])\n\n[2.06270619 2.55152881]\n\n\n\n# Test prediction on a fake point\n\npoint = np.array([[1, 2]]) # should have label 1\nprediction = perceptron.predict(point)\nprint(f\"Prediction for point {point} is {prediction[0]}\")\n\nPrediction for point [[1 2]] is 1\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"#333\", linestyle = \"dashed\")\n\n\ndef plot_history(history, step_size = 1):\n\n    ax = (sns.lineplot(\n            x = range(0, len(history)*step_size, step_size), \n            y = history, \n            errorbar=None,\n         ))\n\n    ax.set_ylabel(\"Accuracy\")\n    ax.set_xlabel(\"Step\")\n    ax.set_title(\"Perceptron Accuracy History\")\n    ax.set_facecolor(\"#fafafa\")\n\n    return ax\n    \n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y) # type: ignore\nfig = draw_line(perceptron.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nObserve that this line is precisely the separating line specified by perceptron.w.\n\ndef plot_simple_scatter(X, y, xlab = \"\", ylab = \"\", title = None,):\n    df = pl.DataFrame({\"x\": X[:,0], \"y\": X[:,1], \"label\": y})\n    ax = (sns.scatterplot(\n        data = df,\n        x = \"x\",\n        y = \"y\",\n        hue = \"label\",\n        palette = [\"g\", \"b\"],\n    ))\n\n    ax.set_xlabel(xlab)\n    ax.set_ylabel(ylab)\n\n    # ax.set_facecolor(\"#fafafa\")\n    ax.set_facecolor(\"#f1f3f5\")\n\n    if title is not None:\n        ax.set_title(title)\n\n    return ax # allows for further customization\n\n\n\nWe know that the perceptron model is guaranteed to converge to a separating line if the data is linearly separable. But what happens if we feed the model data that is not linearly separable?"
  },
  {
    "objectID": "posts/perceptron/index.html#non-linearly-separable-data",
    "href": "posts/perceptron/index.html#non-linearly-separable-data",
    "title": "Perceptron",
    "section": "Non-Linearly Separable Data:",
    "text": "Non-Linearly Separable Data:\n\n# Let's try to fit to a non-linearly separable dataset\n\nseed = 12345 # we are reseeding so we can run each cell independently\nnp.random.seed(seed)\n\nnX, ny = make_blobs(n_samples = 300, n_features = p_features - 1, centers = [(1.7, 0.5), (1.7, 1.7)], cluster_std = 0.5) # type: ignore\n\nax = plot_simple_scatter(nX, ny, xlab = \"Feature 1\", ylab = \"Feature 2\", title = \"Non-linearly separable dataset\")\n\n\n\n\nClearly, this data is not linearly separable. Let’s see what happens when we feed it to the perceptron model.\n\n# Let's try to fit to a non-linearly separable dataset\np = Perceptron()\np.fit(nX, ny, 10000) # type: ignore\n\nFit model in 10000 steps with score 0.8866666666666667\n\n\nSo we converged after max_steps iterations. But we did not converge to a line that perfectly separates the data. Indeed, under these conditions, if we did not feed the model a maximum number of iterations, it would never converge and would instead run forever.\n\n# plot_history(p.history) produces too much noise\nhistory_subset = [p.history[i] for i in range(0, len(p.history), 100)]\n\nax = plot_history(history_subset, 100)\n\n\n\n\nAs you can see the accuracy of the model ‘ping-pongs’ around and is never able to reach far above 0.9. But what if we had a way to short circuit the model when it’s accuracy is ‘good enough’?\nwhile (not self.__has_epsilon_converged(X_, y, epsilon)) and i < max_steps:\n            self.__update_weights(X_, y, self.w, rate)\n            self.__record_history(X_, y)\n            i += 1\nLooking into our loop code, this is exactly what __has_epsilon_converged(X_, y, epsilon) does. Given some \\(\\epsilon\\)-threshold, it checks if the model’s loss is \\(\\leq\\) that threshold. If it is, it returns True and the model stops training. If it is not, it returns False and the model continues training.\nBut this example was trained with \\(\\epsilon = 0\\), meaning it only stops when it reaches 100% accuracy.\n\n# Check accuracy and plot decision boundary\np.score(nX, ny) # type: ignore\n\nax = plot_simple_scatter(nX, ny, xlab = \"Feature 1\", ylab = \"Feature 2\", title = \"Non-linearly separable dataset\")\nax = draw_line(p.w, 0, 4)\n\n\n\n\nLet’s see if we can improve our model by using this new feature.\n\n# Now let's try with epsilon = 0.15\n\nseed = 12345\nnp.random.seed(seed)\n\np = Perceptron()\nlr = 1\ne = 0.15\np.fit(nX, ny, 10000, lr, e) # type: ignore\n\nax = plot_simple_scatter(nX, ny, xlab = \"Feature 1\", ylab = \"Feature 2\", title = \"Non-linearly separable dataset\")\nax = draw_line(p.w, 0, 4)\n\nFit model in 9 steps with score 0.8866666666666667\n\n\n\n\n\nModel short-circuiting works. In 9 steps, we have higher accuracy than the 10k step previous example. With some hyperparameter tuning on \\(\\epsilon\\), we could probably get even better results."
  },
  {
    "objectID": "posts/perceptron/index.html#higher-dimensional-data",
    "href": "posts/perceptron/index.html#higher-dimensional-data",
    "title": "Perceptron",
    "section": "Higher Dimensional Data:",
    "text": "Higher Dimensional Data:\nWe can also use the perceptron model to classify data with more than two features. Let’s see what happens when we feed the model data with five features.\nWhile we lose the ability to (easily) visualize the data, we will still see that the model is able to find a separating line.\n\n# Try higher dimensionality (5 features)\n\nseed = 12345\nnp.random.seed(seed)\n\ncenter1 = np.random.uniform(-2, 2, 5)\ncenter2 = np.random.uniform(-2, 2, 5)\n\nhX, hy = make_blobs(n_samples = 300, n_features = 5, centers = [center1, center2], cluster_std = 0.5) # type: ignore\n\n# Test f1 x f2\nax = plot_simple_scatter(hX[:,[0,1]], hy, xlab = \"Feature 1\", ylab = \"Feature 2\", title = \"Five-dimensional dataset (f1 x f2)\")\n\n\n\n\nThe above (f1 x f2) plot is a 2D projection of the 5D data. This plot is not very useful, but it provides a limited visual representation of the data and interestingly, implies that the data is linearly separable.\n\nseed = 123456 # seed of 12345 converges in 1 step\nnp.random.seed(seed)\n\np = Perceptron()\np.fit(hX, hy, 10000, 1) # type: ignore\n\nFit model in 33 steps with score 1.0\n\n\n\nax = plot_history(p.history)\n\n\n\n\nClearly, our 5D data is linearly separable since we achieve 100% accuracy."
  },
  {
    "objectID": "posts/perceptron/index.html#update-step-time-complexity",
    "href": "posts/perceptron/index.html#update-step-time-complexity",
    "title": "Perceptron",
    "section": "Update-step time complexity:",
    "text": "Update-step time complexity:\n\\[\\tilde{\\mathbf{w}}^{(t+1)} = \\tilde{\\mathbf{w}}^{(t)} + \\mathbb{1}(\\tilde{y}_i \\langle \\tilde{\\mathbf{w}}^{(t)}, \\tilde{\\mathbf{x}}_i\\rangle < 0)\\tilde{y}_i \\tilde{\\mathbf{x}}_i\\;\\]\nThe update step features a dot product between the weights and the input vector. A dot product is O(p) where p is the number of features. The other operations are simple additions and multiplications, which are ~O(1). The dominate operations are O(p), therefore, the update step has an overall time complexity of O(p).\n\nConsider the following weight update function:\n\n    def __update_weights(self, X, y, w, rate) -> None:\n        ''' Internal method to update model weights\n        @param X: 2D array of shape (n_samples, n_features + 1)\n        @param y: 1D array of shape (n_samples) of labels (0 or 1)\n        @param w: 1D array of shape (n_features + 1) of weights\n        @param rate: float of the learning rate\n        @return: None\n        '''\n        index = np.random.randint(0, len(X))\n        y_hat = 2 * y[index] - 1\n\n        self.w = w + int(y_hat * np.dot(X[index], w) < 0) * rate * (X[index] * y_hat)\nThis is the in-code implementation of the update step. We begin by randomly selecting a point from the dataset. We get the label of this point and convert it to a \\(-1\\) or \\(1\\). We then compute the dot product between the weights and the input vector. If the dot product is less than \\(0\\), we update the weights.\nThe updated weight vector is computed by multiplying the learning rate (\\(1\\)) by the input vector and our modified label \\(\\in \\{-1,1\\}\\). We then add this to the existing weights.\nThe only computationally expensive operation is the dot product. All others, including the random index selection, array indexing, type coercion, addition, and multiplication are ~O(1)."
  },
  {
    "objectID": "posts/perceptron/index.html#sources",
    "href": "posts/perceptron/index.html#sources",
    "title": "Perceptron",
    "section": "Sources:",
    "text": "Sources:\n\n[1] Perceptron title-card image"
  },
  {
    "objectID": "posts/unsupervised-learning/index.html",
    "href": "posts/unsupervised-learning/index.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "In this two-part blog post, I will discuss unsupervised learning. In the first part, I will discuss SVD and its application for image compression. And in the second part, I will discuss Spectral Clustering and K-Means Clustering for use in graph partitioning."
  },
  {
    "objectID": "posts/unsupervised-learning/index.html#svd-for-image-compression",
    "href": "posts/unsupervised-learning/index.html#svd-for-image-compression",
    "title": "Unsupervised Learning",
    "section": "SVD for Image Compression",
    "text": "SVD for Image Compression\nSingular-Value Decomposition (SVD) is a matrix factorization technique that can be used to reduce the dimensionality of a matrix. In part 1, I will show how we can exploit dimensionality reduction to compress images.\nWe will start by choosing a benchmark image to work with. I will use the following image of a cheetah since it includes lots of fine detail and high contrast.\nObserve below the detail in\n\nthe cheetah’s fur\nthe grass blades\nthe blurred background\nand in the gradient of the cheetah’s spots as it approaches the face\n\nFor ease of use, I will convert the image to grayscale and then to a numpy array.\n\nimage_url = \"https://fastly.picsum.photos/id/219/1200/1200.jpg?hmac=hVx7D_aNUjlmtlaYidaqbaIOhwFFYwkL1VrMXiFFFCY\"\nimg = read_image(image_url)\n\nimg_grey = ImageCompressionSVD.to_grayscale_image(img)\nImageCompressionSVD.compare_images(img, img_grey, \"original image\", \"grayscale image\")\n\n\n\n\nOur solution compresses images by takeing a subset of size \\(k\\) of the singular values. The following code shows how we can do this using the numpy.linalg.svd function.\n    def compress_image(img, k):\n        u, s, v = np.linalg.svd(img)\n        return u[:, :k] @ np.diag(s[:k]) @ v[:k, :]\nSpecifically, we take:\n\nThe first k columns of U.\nThe top k singular values in S\nThe first k rows of V.\n\nWe also consider an extension to this compression by letting our user specify a value of \\(\\epsilon\\) that filters out singular values that are less than \\(\\epsilon\\).\n    def compress_image(img, k, epsilon = None):\n        u, s, v = np.linalg.svd(img)\n        if epsilon is not None:\n            s[s < epsilon] = 0\n        return u[:, :k] @ np.diag(s[:k]) @ v[:k, :]\nWe can now compare the original image to the compressed image. We will use the ImageCompressionSVD.compare_images method to do this.\n    def compare_images(A, A_, title_1 = \"original image\", title_2 = \"reconstructed image\"):\n        fig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\n        axarr[0].imshow(A, cmap = \"Greys\")\n        axarr[0].axis(\"off\")\n        axarr[0].set(title = title_1)\n\n        axarr[1].imshow(A_, cmap = \"Greys\")\n        axarr[1].axis(\"off\")\n        axarr[1].set(title = title_2)\nThe ImageCompressionSVD.svd_reconstruct method will do the entire compression and comparison pipeline for us.\n\nImageCompressionSVD.svd_reconstruct(img, 1)\n\n\n\n\n\nImageCompressionSVD.svd_reconstruct(img, 15)\n\n\n\n\n\nImageCompressionSVD.svd_reconstruct(img, 1000)\n\n\n\n\nCompression with \\(k=1\\) is unrecognizable.\nSo compressing with \\(k=15\\) doesn’t give us a great image but the image is still recognizable.\nAnd compressing with \\(k=1000\\) gives us a pretty good image.\nBut what exactly is our cost savings here? And how do we choose \\(k\\)?\nWe can compute the relative size of our images mathematically. The following code computes the relative size of the compressed image to the original image.\n    def get_relative_size(A, k):\n        A_space = A.shape[0] * A.shape[1]\n        A_compressed_space = k * (A.shape[0] + A.shape[1]) + k\n        return A_compressed_space / A_space\nIn our compressed image we keep the first \\(k\\) rows of \\(U\\) (which has shape \\(m \\times m\\)), the first \\(k\\) columns of \\(V\\) (which has shape \\(n \\times n\\)), and the first \\(k\\) singular values in \\(S\\) (which has shape \\(m \\times n\\)).\nThis gives us \\(k \\cdot m\\) elements from \\(U\\), \\(k \\cdot n\\) elements from \\(V\\), and \\(k\\) elements from \\(S\\). So our total number of elements is \\(k \\cdot (m + n) + k\\).\nBut how useful is this compression if the images it produces are low-quality?\nTo experiment with our solution, we will run our SVD compression -> decompression pipeline on the image with different values of \\(k\\). We will then compare the original image to the decompressed image for each value of \\(k\\). And we wll observe the relative size of the compressed image to the original image for each value of \\(k\\).\n\nk_values = [10, 50, 100, 200]\nImageCompressionSVD.svd_experiment(img, k_values)\n\nrelative size for k = 10: 0.01667361111111111\n\n\n\n\n\nrelative size for k = 50: 0.08336805555555556\n\n\n\n\n\nrelative size for k = 100: 0.1667361111111111\n\n\n\n\n\nrelative size for k = 200: 0.3334722222222222\n\n\n\n\n\nSo we can see the images are about identical at \\(k=200\\) and the relative size is about \\(\\frac{1}{3}\\) of the original image! This means we can compress our image by a factor of 3 and still have a very high quality image.\nNow what if we need to compress our image by a certain amount? We could test a bunch of \\(k\\) values ourselves but that would be tedious. Instead, we can use the ImageCompressionSVD.find_k_for_compression_threshold method to do this for us. Since this our relative-size method uses only simple math, it is very fast. If we were even more concerned about speed, we could switch to using binary search instead of linear search.\nSo, if I need to make an image 3x smaller, I can call find_k_for_compression_threshold with a compression_factor of 3 and it will return the value of \\(k\\) that will give me a compressed image that is 1/3 the size of the original image.\n\n# I want to make the image 3x smaller\ncompression_factor = 3\nk = ImageCompressionSVD.find_k_for_compression_threshold(img, compression_factor)\nprint(f\"{k = }\",)\n\nthreshold: 0.3333333333333333\nk = 199\n\n\nHere we can see that value of \\(k\\) is 199. So as evidenced above, we can compress our image by a factor of 3 and still have a very high quality image.\n\n\nShow the code\nImageCompressionSVD.svd_reconstruct(img, k)\n\n\n\n\n\nNow we can also use our \\(\\epsilon\\) parameter to filter out singular values that are less than \\(\\epsilon\\). This could be used to additionally improve our compression (if we are using sparse matrix formats).\n\n# I want to ignore all singular values less than epsilon\n\nepsilon = 1e-4\nImageCompressionSVD.svd_reconstruct(img, k, epsilon)\n\n\n\n\nBut we do need to be careful, consider what happens if we use an epsilon value that is too large. Observe the following image.\n\n# What happens if epsilon is too large?\n\nepsilon = 20000\n\nImageCompressionSVD.svd_reconstruct(img, k, epsilon)\n\n\n\n\nRecall this similar to the initial compression examples with \\(k=1\\)"
  },
  {
    "objectID": "posts/unsupervised-learning/index.html#spectral-clustering",
    "href": "posts/unsupervised-learning/index.html#spectral-clustering",
    "title": "Unsupervised Learning",
    "section": "Spectral Clustering",
    "text": "Spectral Clustering\nIn part 2, I will discuss Spectral Clustering for use in graph partitioning. We will start with the famous Karate Club graph.\n\n\nShow the code\nG = nx.karate_club_graph()\nlayout = nx.layout.fruchterman_reingold_layout(G)\nnx.draw(G, layout, with_labels=True, node_color = \"steelblue\")\n\n\n\n\n\nWe will use the club label to denote the true partition of the graph. As shown in the figure below, the graph has 2 communities and split it into 2 roughly equal parts.\n\n\nShow the code\nclubs = nx.get_node_attributes(G, \"club\")\nnx.draw(G, layout,\n        with_labels=True, \n        node_color = [\"orange\" if clubs[i] == \"Officer\" else \"steelblue\" for i in G.nodes()],\n        edgecolors = \"black\" # confusingly, this is the color of node borders, not of edges\n        ) \n\n\n\n\n\nWe will use the SpectralCommunityDetection class to do the clustering. The SpectralCommunityDetection.spectral_clustering method will do the entire clustering pipeline for us.\n    def get_clusters(cls, G, k):\n        L = cls.get_laplacian_matrix(G)\n        X = cls.get_eigenvectors(L, k)\n        return cls.k_means_clustering(X, k)\n    \n    def spectral_clustering(cls, G, k=2):\n        clusters = cls.get_clusters(G, k)\n        return cls.build_community_label_vector(clusters, k)\nTo do the clustering, we will use the following steps:\n\nGet the Laplacian matrix of the graph.\nGet k eigenvectors sorted by eigenvalue from the Laplacian matrix.\nCluster the eigenvectors using k-means into k clusters.\nOutput a vector of labels in the same order as the nodes in the graph.\n\nWe will use the SpectralCommunityDetection.get_laplacian_matrix method to get the Laplacian matrix of the graph.\n    def get_laplacian_matrix(cls, G):\n        A = cls.get_adjacency_matrix(G)\n        return np.diag(A.sum(axis=0)) - A\nThe Laplacian matrix is defined as \\(L = D - A\\) where \\(D\\) is the diagonal degree matrix and \\(A\\) is the adjacency matrix.\nThen we can get the eigenvectors of the Laplacian matrix using the SpectralCommunityDetection.get_eigenvectors method.\n    def get_eigenvectors(L, k):\n        e_values, e_vectors = np.linalg.eig(L) \n\n        e_values = np.real(e_values) # We need to discard imaginary parts for kmeans\n        e_vectors = np.real(e_vectors)\n\n        idx = np.argsort(e_values)[:k]\n        return e_vectors[:, idx]\nAnd we can cluster the eigenvectors using k-means using the SpectralCommunityDetection.k_means_clustering method. We will use the sklearn implementation of k-means.\n    def k_means_clustering(X, k):\n        kmeans = KMeans(n_clusters=k).fit(X)\n        return kmeans.labels_\nAnd finally we can build a vector of labels in the same order as the nodes in the graph using the SpectralCommunityDetection.build_community_label_vector method.\n    def build_community_label_vector(clusters, k):\n        # [[0, 1, 2], [3, 4, 5]] -> [0, 0, 0, 1, 1, 1]\n        community_labels = np.zeros(len(clusters)).astype(int)\n        for i in range(k):\n            community_labels[clusters == i] = i\n        return community_labels\n\ncommunities = SpectralCommunityDetection.spectral_clustering(G, 2).tolist()\nnx.draw(G, layout,\n        with_labels=True,\n        node_color = [\"orange\" if communities[i] == 1 else \"steelblue\" for i in G.nodes()],\n        edgecolors = \"black\" # confusingly, this is the color of node borders, not of edges\n)\n\nNone # suppress output\n\n\n\n\nWe make the classic mistake of mislabeling individual 8. But overwise, our labeling is very good. We can see that the clustering algorithm has correctly identified the 2 communities in the graph.\nNow let’s see how we can extend spectral clustering to scatterplots of points. Since we defined spectral clustrering over a graph, we can transform our points into a graph using the get_nearest_neighbor_graph method. This method uses KNN to find the nearest neighbors (in Euclidean distance) on the scatterplot and sets them as neighbors in the graph. We can then use the same spectral clustering algorithm to cluster the points.\n\n\n\n\n\nBut now we need to be careful as our constructed graph relies on a hyperparameter, K. As shown below, a wrong choice of K can lead to an obvious misclassification of points.\n\n\nShow the code\nnp.random.seed(0)\nGC = SpectralCommunityDetection.get_nearest_neighbor_graph(X, 6)\ncommunities = SpectralCommunityDetection.spectral_clustering(GC, 2).tolist()\n\ncolors = [\"orange\", \"steelblue\"]\n\nnx.draw_networkx_nodes(\n    GC,\n    pos = {i: X[i] for i in range(len(X))},\n    node_color = [colors[i] for i in communities],\n    node_size = 2\n)\n\nNone\n\n\n\n\n\nNow let’s show that we can use this algorithm with a better choice of K and that we can use it to cluster multiple classes of points. We will do this by adding two more circles to our scatterplot from sklearn.datasets.make_circles.\n\n\nShow the code\n# 4 rings\n\nX_large, y_large = make_circles(n_samples=1000, noise=0.05, factor=0.5)\nX_small, y_small = make_circles(n_samples=1000, noise=0.05, factor=0.2)\n\nX_large[:, 0] *= 3.5\nX_large[:, 1] *= 3.5\nX_small[:, 0] *= 0.6\nX_small[:, 1] *= 0.7\ny_small += 2\n\nX = np.concatenate([X_large, X_small])\ny = np.concatenate([y_large, y_small])\n\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.show()\nNone\n\n\n\n\n\nObserve the 4 distinct classifications of points. We want our algorithm to return a classification of points that looks like this.\n\nnp.random.seed(0)\nGMC = SpectralCommunityDetection.get_nearest_neighbor_graph(X, 12)\ncommunities = SpectralCommunityDetection.spectral_clustering(GMC, 4).tolist()\n\n\ncolors = [\"orange\", \"steelblue\", \"green\", \"red\"]\n\nnx.draw_networkx_nodes(\n    GMC,\n    pos = {i: X[i] for i in range(len(X))},\n    node_color = [colors[i] for i in communities],\n    node_size = 2\n)\n\n<matplotlib.collections.PathCollection at 0x15f8c41a7a0>\n\n\n\n\n\nWhile this looks good to my eyes, how can we quantitatively determine if this labeling is good?\nUnfortunately, the classes returned by our algorithm do not have the same numerical identifiers as the classes in the original dataset. So we need to find a way to evaluate the quality of our clustering. We will do this by using a similarity measure.\nFor a measure of similarity \\([0, 1]\\) between two categorical labelings \\(z_1\\) and \\(z_2\\) that is permutation invariant, I propose we consider the averaged Jaccard Index about each point \\(i\\): such that \\(sim(z_1, z_2) = \\frac{1}{n}\\sum_{i=1}^n J(label(z_1, i), label(z_2, i))\\)\nwhere \\(label(z_i, j)\\) = returns the label group \\(g_{i,j}\\) of point \\(j\\) in \\(z_i\\). and where \\(J(g_{i,j}, g_{n, j})\\) is the Jaccard Index between the group that contains point \\(j\\) in \\(z_i\\) and the group that contains point \\(j\\) in \\(z_n\\).\n\ndef jaccard_index(gi: list, gj: list):\n    \"\"\"compute the jaccard index between two node label groups\"\"\"\n    return len(set(gi).intersection(set(gj))) / len(set(gi).union(set(gj)))\n\n\ndef averaged_jaccard_index_sim(labels:list, zi: list, zj: list):\n    \"\"\"compute the average jaccard index between node label groups in zi and zj\"\"\"\n\n    zi_node_to_group_map = {node: group for node, group in enumerate(zi)} # exactly zi\n    zj_node_to_group_map = {node: group for node, group in enumerate(zj)}\n\n    zi_group_to_node_map = {group: [] for group in set(labels)}\n    zj_group_to_node_map = {group: [] for group in set(labels)}\n\n    for node, group in enumerate(zi):\n        zi_group_to_node_map[group].append(node)\n    \n    for node, group in enumerate(zj):\n        zj_group_to_node_map[group].append(node)\n\n    jaccard_index_sum = 0\n    for node, _ in enumerate(zi):\n        zi_group = zi_node_to_group_map[node]\n        zj_group = zj_node_to_group_map[node]\n        zi_node_list = zi_group_to_node_map[zi_group]\n        zj_node_list = zj_group_to_node_map[zj_group]\n\n        jaccard_index_sum += jaccard_index(zi_node_list, zj_node_list)\n\n    return jaccard_index_sum / len(zi)\n\n\n\n\nAs an example of this measure, let’s consider the Karate Club example from earlier.\n\nactual_labeling = [0 if clubs[i] == \"Officer\" else 1 for i in G.nodes()]\npredicted_labeling = SpectralCommunityDetection.spectral_clustering(G, 2)\nlabels = [0, 1]\n\nsim = averaged_jaccard_index_sim(labels, actual_labeling, predicted_labeling)\n\n\n# Test label invariance\nactual_labeling = [1 if clubs[i] == \"Officer\" else 0 for i in G.nodes()]\npredicted_labeling = SpectralCommunityDetection.spectral_clustering(G, 2)\nlabels = [0, 1]\n\nsim2 = averaged_jaccard_index_sim(labels, actual_labeling, predicted_labeling)\n\nprint(f\"{sim = :.3f}\")\nif np.isclose(sim, sim2):\n    print(\"Label invariance works!\")\n\nsim = 0.916\nLabel invariance works!\n\n\nSo our similarity measure is label invariant and is defined on \\([0,1]\\). A high similarly score means that two labeling are similar. So this measure confirms that our spectral clustering is good at classifying the communities in the Karate Club graph.\nNow let’s use this measure to see how well we did on the rings of circles example.\n\nactual_labeling = y\npredicted_labeling = SpectralCommunityDetection.spectral_clustering(GMC, 4)\nlabels = [0, 1, 2, 3]\n\nsim = averaged_jaccard_index_sim(labels, actual_labeling, predicted_labeling)\nprint(f\"{sim = :.3f}\")\n\nsim = 1.000\n\n\nOur spectral clustering is very good at classifying the communities in our rings of circles graph (with the added caveat that we needed to fiddle with the K value during the KNN graph construction step)"
  }
]